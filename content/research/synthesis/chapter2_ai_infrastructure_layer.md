## Chapter 2 – Compute Empires: The Strategic Architecture of AI Infrastructure



AI infrastructure has emerged as the decisive layer of geopolitical competition. Beneath model development and applications lie the physical and virtual substrates—chips, interconnects, memory packaging, cloud data centers, and power supply chains—that determine who can build frontier systems. By 2025, this layer is stratified across three models: vertically integrated monopolies led by NVIDIA and hyperscalers; open-standard coalitions organized around AMD and Linux Foundation initiatives; and sovereign-specialized designs such as Groq, Cerebras, and Huawei. These architectures embed political economies into circuits and cooling loops, transforming technical design into geopolitical leverage.

AI infrastructure refers to more than just hardware. It encompasses semiconductors, interconnect technologies, high-bandwidth memory, cooling and energy arrangements, cloud orchestration, and compiler toolchains. Each element is a chokepoint, often dominated by a handful of firms whose decisions ripple across the ecosystem. Unlike earlier phases of computing, switching between stacks is prohibitively costly. An organization trained on CUDA-optimized GPUs is effectively locked into NVIDIA’s cadence, while adopting an alternative requires redesigning software pipelines and retraining staff. This creates path dependency that mirrors feudal allegiance: once inside a stack, exit is difficult without massive capital outlay.

NVIDIA exemplifies the vertically integrated model. Its H100 and B200 series GPUs, combined with the CUDA software stack and proprietary NVLink interconnects, dominate training-scale compute. By bundling chip design, compiler optimization, and data-center reference architectures, NVIDIA offers turnkey systems that hyperscalers resell under GPU-as-a-service pricing. The result is a self-reinforcing monopoly: developers optimize for CUDA because it is dominant, and CUDA is dominant because developers optimize for it. Export controls amplify this power by making NVIDIA chips instruments of U.S. foreign policy, with restrictions on sales to China, the Gulf, and other strategic regions.

An alternative has emerged around AMD and coalition-based initiatives. AMD’s Instinct MI300X and MI350X series, coupled with the open-source ROCm stack and participation in the Ultra Ethernet Consortium, represent a push toward interoperability. This model attracts actors wary of NVIDIA dependency: Meta has deployed MI300X GPUs for Llama inference, while Oracle Cloud has committed to building zettascale clusters around them. Yet the ecosystem remains fragmented. ROCm lags behind CUDA in developer adoption, tooling, and optimization. The promise is political flexibility—no single vendor lock-in—but at the cost of performance ceiling and fragmented standards.

The third trajectory lies in sovereign-specialized architectures. Cerebras’s wafer-scale engine is designed for ultra-large model training, Groq’s deterministic inference processors provide millisecond latency for real-time systems, and Huawei Ascend chips act as hedges against U.S. export restrictions. These designs are not general-purpose competitors but strategic bets: wafer-scale processing for scientific workloads, low-latency inference for defense and finance, national silicon stacks for sovereignty. Their viability often depends on state subsidy or targeted procurement. Saudi Arabia’s $1.5B investment in Groq illustrates how capital and security policy intersect to guarantee access to critical capabilities outside U.S. control.

Cloud hyperscalers constitute another axis of infrastructural power. AWS, Microsoft Azure, Google Cloud, and Oracle Cloud control access to the vast GPU clusters required for model training. Each follows a distinct strategy: AWS presents itself as a neutral utility, Azure as a vertically integrated ecosystem tied to OpenAI, and Google Cloud as an innovation-driven TPU platform. What they share is concentration: training-scale compute is effectively rationed by four actors. For smaller labs, access depends less on innovation than on the ability to negotiate contracts or secure subsidies. This concentration translates into strategic leverage: hyperscalers determine not only cost but who gets to experiment at the frontier at all.

Edge computing broadens the field but replicates the same dynamics at smaller scales. Qualcomm’s Snapdragon X Elite powers hybrid AI PCs capable of running 10–13B parameter models locally; Apple’s M4 Neural Engine integrates AI into consumer devices under a vertically locked ecosystem; NVIDIA’s Jetson Thor serves industrial robotics with embedded high-performance chips. Each of these devices extends the reach of AI infrastructure into personal electronics, but under architectures optimized for lock-in and proprietary ecosystems. What looks like decentralization is in fact another venue for ecosystem control.

The political economy of infrastructure is inseparable from physical constraints. Advanced packaging technologies such as CoWoS, required for high-bandwidth memory integration, are controlled by a single firm—TSMC—with long lead times and strategic exposure to geopolitical shocks in Taiwan. HBM supply itself is dominated by three firms—SK Hynix, Samsung, and Micron—whose capacity is already backlogged. Energy and cooling form further chokepoints: frontier clusters draw power equivalent to medium-sized cities, prompting firms like Microsoft and Amazon to pursue nuclear power deals or water rights acquisitions. Infrastructure is thus not only a technical stack but a territorial claim on scarce resources.

This first half of the chapter has traced the structural models of AI infrastructure—vertical integration, open coalitions, and sovereign-specialized designs—while outlining their dependency chains and strategic bottlenecks. The second half will examine the risks embedded in these architectures, their role in geopolitical contestation, and the governance dilemmas they pose for states and institutions attempting to regulate infrastructure that is already concentrated, globalized, and fragile.

The failure modes of AI infrastructure are as consequential as its capabilities. One risk is monopolistic dependency: the dominance of NVIDIA and its CUDA ecosystem creates a systemic single point of failure. If export controls or supply disruptions constrain availability, the entire global training pipeline is affected. A second risk is standard fragmentation. Competing stacks—CUDA, ROCm, TPU, wafer-scale engines—lack interoperability. This prevents redundancy across supply chains, raising the cost of switching and complicating international collaboration.

Energy and ecological stresses form another class of risks. Training frontier models consumes tens of gigawatt-hours, straining regional grids and diverting water for cooling. Municipalities in Arizona, Ireland, and the Netherlands have already raised restrictions on new data centers due to energy scarcity. These pressures intensify global competition for clean energy inputs, positioning nuclear and hydroelectric capacity as critical enablers of AI sovereignty. The environmental externalities of compute empires—resource extraction for rare earths, water use, and e-waste—are rarely internalized in cost calculations but increasingly visible in political backlash.

Security vulnerabilities also concentrate in the infrastructure layer. Data centers housing GPUs and TPUs are obvious cyber targets, while the global reliance on TSMC exposes the ecosystem to geopolitical shocks around Taiwan. Export controls amplify the risk of gray-market diversion: smuggling of restricted chips through intermediaries undermines intended policy goals while fueling clandestine deployments. Sovereign chip projects in China, Russia, and the Middle East emerge precisely from the fear of infrastructural exclusion, but these projects often replicate earlier-generation architectures, locking them into a technological lag.

Governance over this domain is still rudimentary. States regulate export flows, competition policy, and environmental impacts, but these tools are reactive. Supranational attempts—such as EU industrial policy around sovereign cloud and the U.S.–Japan–Netherlands semiconductor alliance—illustrate how geopolitical blocs seek to exert control through industrial strategy. Yet enforcement remains partial. Export restrictions are porous, environmental regulations vary by jurisdiction, and antitrust enforcement lags behind the consolidation tempo of hyperscalers. The result is fragmented governance over an integrated global stack.

One governance dilemma is whether to treat compute infrastructure as a public utility or a competitive commodity. If understood as utility, states would need to guarantee equitable access, impose capacity allocation rules, and intervene in pricing. If understood as commodity, innovation would remain dependent on market forces and the private bargaining power of hyperscalers. Both approaches carry trade-offs: public utility risks slowing innovation, while commodity logic entrenches private monopolies and exacerbates inequality of access.

A second dilemma is international regulation. The World Trade Organization and G7 frameworks attempt to coordinate export rules, but compute restrictions are increasingly unilateral, deployed as instruments of industrial rivalry. This hardens divides: U.S. allies gain privileged access, while competitors are excluded and compelled to develop parallel ecosystems. In effect, infrastructure becomes a geopolitical frontier, where technical specifications encode alliances.

A third dilemma is environmental accountability. Frontier AI cannot be disentangled from its ecological footprint, yet few governance frameworks integrate sustainability with compute policy. Proposals for carbon disclosure in AI training, mandatory energy audits for hyperscalers, or shared nuclear co-investments remain at the level of experiment. Without coordinated governance, infrastructural expansion risks running into ecological ceilings before political ones.

The geopolitical stakes are clear. Compute capacity determines not just who can train state-of-the-art models but who defines the epistemic boundaries of knowledge. Monopolies in chips, clouds, and energy inputs grant firms and states disproportionate leverage over the direction of technological development. Export controls, industrial policy, and infrastructure finance are no longer marginal—they are the central tools through which AI power is contested.

This chapter has mapped the architectures and dependencies of AI infrastructure as a strategic domain. Where Chapter 1 showed divergence at the model layer, Chapter 2 demonstrates that such divergence rests on highly concentrated and fragile infrastructural bases. The transition to the next chapter will follow this dependency chain downward again—into the capital flows, financial architectures, and investment vehicles that make such vast infrastructural empires possible in the first place.