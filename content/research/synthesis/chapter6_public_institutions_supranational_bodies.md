# Title: Governing with Borrowed Tools: Public Institutions and AI Power

# 

# 

# Public institutions and supranational bodies are increasingly tasked with managing, regulating, or deploying artificial intelligence. Yet most lack the infrastructure, expertise, and leverage to do so on their own terms. As a result, their engagement with AI is defined not by sovereign control but by strategic dependence—on private vendors, outsourced infrastructure, and external expertise.

# 

# This chapter examines how governments, international organizations, and regulatory agencies interact with the AI ecosystem as clients, rulemakers, and dependents. It traces their structural constraints, evaluates current governance strategies, and analyzes the growing tension between institutional ambition and infrastructural reality.

# 

# Defining the Institutional Layer

# 

# This layer includes all state and supra-state actors involved in AI regulation, procurement, deployment, and public-sector applications. These range from national ministries and data protection authorities to intergovernmental agencies and transnational rule-setting bodies.

# 

# Unlike the private actors who control models, infrastructure, or applications, public institutions enter the AI field with mandates shaped by democratic processes, legal norms, and public accountability. Their goals—fairness, transparency, nondiscrimination, service efficiency—are often misaligned with commercial incentives.

# 

# Institutional Modes of Engagement

# 1\. Procurement and Platform Dependence

# 

# Governments increasingly acquire AI capabilities not by building them in-house but by purchasing access to commercial platforms. From Microsoft Azure’s integration into EU digital infrastructure to U.S. federal agencies adopting OpenAI via GSA contracts, public-sector engagement often takes the form of platform-as-service licensing.

# 

# This arrangement affords short-term functionality but erodes long-term sovereignty. Institutions become reliant on external actors for model behavior, infrastructure availability, and update cadence. Even when governments attempt to build sovereign systems, they often depend on vendor-hosted APIs or closed toolchains.

# 

# 2\. Regulatory Standard-Setting

# 

# Institutions have made significant strides in defining the outer bounds of permissible AI behavior. The EU AI Act, Brazil’s draft PL 21/2020, and Canada's AIDA framework aim to establish horizontal risk-based approaches. Meanwhile, agencies like the FTC, CNIL, and EDPS are crafting domain-specific enforcement protocols.

# 

# However, these efforts often lag behind deployment realities, especially in fast-moving domains like generative AI. Rulemaking is slow, and enforcement mechanisms are fragmented across jurisdictions. Many institutional actors are forced to govern after the fact, without real-time observability or access to model internals.

# 

# 3\. Delegated Governance and Co-Regulatory Models

# 

# Faced with technical complexity and institutional limitations, many governments are experimenting with co-regulatory approaches: relying on industry self-assessments, third-party audits, or public-private councils. These frameworks aim to blend agility with oversight, allowing regulators to shape norms without owning the stack.

# 

# But co-regulation risks outsourcing critical judgment to actors with misaligned incentives. If red-teaming, evals, and model documentation are conducted by the same firms that deploy the systems, accountability becomes circular. Transparency is promised, not verified.

# 

# Governing with Borrowed Tools: Public Institutions and AI Power

# 

# Institutional engagement with AI is shaped less by ambition than by constraint. Even where political will exists, public agencies face structural limitations that undermine effective governance.

# Few institutions maintain the in-house technical expertise needed to interrogate, evaluate, or even meaningfully configure the AI systems they adopt. Regulatory bodies often rely on external consultants, leading to a knowledge asymmetry that favors vendors. Auditability, red-teaming, and interpretability may be formally required—but practically outsourced.

# The institutional landscape is fragmented across national, regional, and sectoral lines. Cross-border deployments of AI systems often fall into legal grey zones. A chatbot deployed by a U.S. company on a European platform might engage users in multiple languages under different data regimes, none of which has a complete enforcement mandate.

# Public procurement cycles are slow, risk-averse, and often optimized for cost over auditability or adaptability. Once a vendor is chosen, switching becomes politically and technically expensive. This creates path dependency, especially when services are bundled across infrastructure, model access, and orchestration layers.

# When institutions integrate AI tools into public service delivery—immigration screening, social benefits triage, predictive policing—they expose themselves to reputational and legal risk. But retreat is rarely an option. Systems, once deployed, become entangled with core functions. Corrections may be issued, but rollback is structurally disincentivized.

# Rather than viewing institutions as mere laggards or regulators, this chapter reframes them as actors navigating dependence. Their agency is real—but often constrained by design decisions made elsewhere.

# Most public institutions do not have meaningful access to the models they are expected to govern. They cannot inspect the data, control the architecture, or simulate edge-case behaviors at scale. This undermines the plausibility of safety oversight and places them in a reactive enforcement posture.

# Many AI systems deployed in public settings run atop proprietary infrastructure. Whether in schools, courts, hospitals, or municipal services, public actors operate within architectures they do not control. Their values—fairness, due process, equity—must be implemented via APIs designed for throughput and scale, not democratic accountability.

# Red-teaming and evals should not remain the domain of corporate labs or security contractors. Institutions, civil society, and affected communities must be included in the definition of harm thresholds, robustness benchmarks, and acceptable behavior boundaries. This requires building public capacity for adversarial testing, not just mandating private disclosure.

# 

# If institutions are to govern AI effectively, they must not only write rules—they must be able to simulate, audit, and deploy AI systems themselves. This implies investment in public compute, model governance labs, and infrastructure for safe experimentation. Without such capacity, governance becomes performative.

# 

# 

# Public institutions face a paradox: they are tasked with ensuring AI systems serve the public interest, but must do so from a position of structural dependency. They write rules for systems they do not own, procure tools they cannot inspect, and bear responsibility for outcomes they cannot fully control.

# 

# As we turn to Chapter 7, the focus will shift from public actors to private capital—mapping how strategic investment decisions shape the ecosystem from above. From venture funding and sovereign wealth to GPU speculation and equity control, capital exerts a form of governance that is no less powerful than law.

