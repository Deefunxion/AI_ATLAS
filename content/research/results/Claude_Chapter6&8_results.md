# Global AI Governance Landscape: Comprehensive Analysis (January 2024 - August 2025)

## Policy Matrix

| Jurisdiction/Body | Instrument | Scope | Affects (Models/Apps/Data) | Timeline | Enforcement | Source |
|-------------------|------------|--------|---------------------------|----------|-------------|--------|
| **EU/European Commission** | EU AI Act + Delegated Acts | High-risk AI systems, GPAI models | Models >10^25 FLOP, High-risk apps, Training data | Aug 2024-2027 phased | Up to 3% global revenue or €15M | Official Journal L 199, July 12, 2024 |
| **EU/AI Office** | GPAI Code of Practice | General-purpose AI models | All GPAI providers, Systemic risk models | July 2025 published, Aug 2025 effective | Information requests, model recall | Commission Decision Jan 24, 2024 |
| **US/OMB (Biden)** | M-24-10 AI Governance | Safety/rights-impacting AI | Federal agency AI systems | Dec 2024 compliance | Agency compliance requirements | OMB M-24-10, March 28, 2024 |
| **US/OMB (Trump)** | M-25-21 AI Innovation | High-impact AI systems | Federal AI procurement/use | April 2026 extended deadline | Reduced bureaucratic oversight | OMB M-25-21, April 3, 2025 |
| **US/NIST** | AI RMF GenAI Profile | Generative AI systems | Foundation models, GAI applications | July 2024 published | Voluntary guidance framework | NIST AI 600-1, July 26, 2024 |
| **China/CAC** | Interim Measures GenAI | Public-facing AI services | LLMs, generative AI platforms | Aug 2023, enforced 2024-25 | Up to RMB 100K, criminal liability | CAC Order No. 12, July 13, 2023 |
| **China/CAC** | Algorithm Filing System | Recommendation algorithms | Social media, content platforms | March 2022, expanded 2024-25 | 1,400+ algorithms filed by 450+ companies | Algorithm Recommendation Provisions |
| **UK/DSIT** | AI Safety Institute | Frontier AI systems | Advanced AI models, safety testing | Nov 2024 permanent establishment | Technical evaluation, international cooperation | Bletchley Park launch, Nov 2024 |
| **Canada** | Bill C-27/AIDA | High-impact AI systems | AI systems affecting decisions | **Died Jan 6, 2025 (prorogation)** | Would have included criminal penalties | Bill C-27, Parliament of Canada |
| **Japan/METI** | AI Promotion Act | AI development/utilization | All AI technologies | May 28, 2025 enacted | Soft law, cooperation requirements | Act No. X of 2025 |
| **India/MeitY** | AI Governance Framework | AI systems (developing) | Personal data AI, platform AI | Framework development ongoing | Advisory approach currently | MeitY consultations 2024-25 |

## Position Matrix

| Organization | Topic | Stance | Evidence | Conflicts/Bias | Source |
|--------------|--------|---------|----------|---------------|--------|
| **Center for AI Safety** | Binding AI Prohibitions | Strong support for legal "red lines" | CAIS Action Fund co-sponsored CA SB 1047 | Industry funding concerns | 7.25/10 consensus score, 2024 |
| **AI Now Institute** | Corporate AI Power | Critical of industry consolidation | "AI companies spending millions to get laws they want" | Anti-tech industry stance | Co-Directors Kak & West, 2024 |
| **Stanford HAI** | Evidence-based Policy | Support for technical working groups | Trained 3,500+ gov employees in 2024 | Academic-industry partnerships | 2025 AI Index Report |
| **Algorithmic Justice League** | Prohibit Harmful Uses | Focus on "impermissible use" vs bias mitigation | #NoCaseNoFace campaign vs IRS ID.me | Rights-advocacy orientation | Joy Buolamwini, TIME 100 AI, 2024 |
| **Access Now** | EU AI Act Critique | "Failure for human rights" due to exemptions | Law enforcement/migration loopholes | Human rights advocacy bias | #ProtectNotSurveil coalition |
| **Data for Black Lives** | Community-Controlled AI | AI as "tool of liberation" not surveillance | D4BL III "Ustopia" conference theme | Anti-surveillance positioning | Miami conference, Nov 2024 |
| **European AI Office** | Voluntary Compliance First | GPAI Code of Practice over regulation | 1,000+ stakeholders in development process | EU institutional bias | July 10, 2025 Code publication |
| **Trump Administration OSTP** | Innovation-First Deregulation | Remove "bureaucratic restrictions" | America's AI Action Plan three pillars | Pro-industry positioning | July 2025 strategy document |
| **China CAC** | State-Directed Safety | AI safety serves "socialist values" | 2,841 deep synthesis filings approved | Authoritarian control bias | TC260 AI Safety Framework |
| **UK AI Safety Institute** | Technical Safety Leadership | Focus on frontier AI evaluation | World's first government AI Safety Institute | Technical over social approach | Bletchley Park establishment |

## Comparative Analysis: Institutional vs. Civil Society Alignment

The January 2024-August 2025 period reveals **fundamental tensions** between governmental approaches emphasizing innovation and economic competitiveness versus civil society demands for binding human rights protections and democratic accountability.

**Areas of Convergence** center on the need for mandatory disclosure requirements and independent verification mechanisms. The EU's General Purpose AI Code of Practice, developed with nearly 1,000 stakeholders, represents the most successful multi-stakeholder initiative, while academic institutions like Stanford HAI have found common ground with policymakers through evidence-based training programs. Civil society organizations achieved a rare 7.25/10 consensus score supporting "legally binding red lines" for unacceptable AI applications—a position increasingly reflected in the EU AI Act's prohibition categories and China's algorithm filing requirements.

**Critical Divergences** emerge around definitions of "AI safety" and enforcement mechanisms. Government institutions consistently frame safety in technical terms—the US NIST GenAI Profile identifies 12 risk categories focused on system performance, while the UK AI Safety Institute emphasizes frontier model evaluation. **Civil society organizations fundamentally reject this framing**, with AI Now Institute arguing that current approaches ignore "structural power analysis" and Data for Black Lives advocating that some AI applications should be "prohibited entirely" rather than merely regulated.

**The most significant disconnect** involves enforcement philosophy. The Trump administration's shift toward "innovation-first deregulation" exemplifies institutional capture concerns raised by civil society, with OMB M-25-21 explicitly removing "bureaucratic restrictions" while extending compliance deadlines. Conversely, organizations like the Algorithmic Justice League demand immediate prohibitions on facial recognition in essential services, creating an unbridgeable gap between regulatory timelines and community needs.

**Global South perspectives remain systematically excluded** from major governance frameworks despite civil society advocacy. Research identified this as a critical gap, with Nigerian and Indian experts noting that AI governance "risks overlooking socio-economic contexts" when developed without meaningful Global South participation.

The period demonstrates that while technical safety research has achieved integration with policy institutions through organizations like CAIS and UK AI Safety Institute partnerships, **rights-based civil society advocacy remains largely marginalized** in formal governance structures. Access Now's characterization of the EU AI Act as setting "the lowest bar possible" for human rights protection reflects broader civil society frustration with institutional approaches that prioritize industry interests over democratic accountability in AI development.